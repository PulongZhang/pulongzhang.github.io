---
layout: article
title: 从 DeepSeekMath-V2 看 AI Agent/Workflow 的优化思路
key: 20251204-deepseekmath-v2-agent-optimization
tags: AI Agent Tech
author: Gemini
aside:
  toc: true
typora-root-url: ./..
---

在之前的讨论中，我们对 AI Agent/Workflow 在企业领域软件开发层面为什么表现不好已经有一定的认识。然而，领导的意思是不能违抗的，他始终如一地坚持走"人机分离"的路线，并要求我们给出方案，我只能去寻找和构思可能的方案。

<!--more-->

恰好此时，[DeepSeekMath-V2](https://github.com/deepseek-ai/DeepSeek-Math-V2) 模型发布了。我在了解此模型的时候也想到了我的工作，**调优 AI Agent 和训练模型似乎是类似的思路**：Agent/Workflow/思维树 (Tree of Thoughts) 就像模型的网络结构，而模型的权重参数又类似 Agent 中的提示词。从这个角度出发，我开始仔细了解 DeepSeekMath-V2 的相关细节。

### 研究背景与核心痛点

传统的数学模型训练往往只关注"最终答案是否正确" (Outcome Reward)，但这在定理证明 (Theorem Proving) 中是行不通的，因为证明过程的逻辑严密性才是关键。

DeepSeekMath-V2 的核心突破在于**不仅仅训练模型做题，还训练模型像人类专家一样"自我找茬"和"自我修正"**。

**核心痛点：**

- 现有的模型即便答案对了，推理过程也可能是错的（瞎蒙）。
- 在面对没有标准答案的开放性证明题时，无法通过最终答案来获得奖励信号。

**解决思路：**

- 建立一个"生成-验证"的闭环。让模型学会**自我验证 (Self-Verification)**，即自己检查自己的证明步骤，找出逻辑漏洞并修复，从而实现从"结果正确"到"过程严密"的跨越。

那么，DeepSeekMath-V2 是如何做到的呢？

### 训练机制：生成与验证的协同进化

DeepSeekMath-V2 的训练过程可以分为三个关键部分：**验证器训练**、**生成器训练**以及**两者协同**。

#### A. 验证 (Verification)

这是系统的基石，模型需要学会评价一个证明的好坏。

1. **评分标准：** 设定了三个等级。
   - **1分：** 完美、严谨的证明。
   - **0.5分：** 逻辑大体正确，但有小错误或步骤跳跃。
   - **0分：** 根本没解决问题或有重大逻辑漏洞。

2. **引入"元验证" (Meta-Verification)：** 这是该方法的一大亮点。
   - **问题：** 训练初期的验证器容易"幻觉"，明明证明是对的，它非要挑刺说有错 (False Positive)。
   - **解决：** 训练一个 **Meta-Verifier（元验证器）** 来审查"验证器的评价"是否合理。如果验证器瞎挑刺，Meta-Verifier 会给低分。
   - **效果：** 通过元验证的反馈，验证器的评价变得非常客观、可信，不再胡乱报错。

#### B. 生成 (Generation)

有了好的验证器，就可以训练生成器了。

1. **自我验证机制 (Self-Verification)：**
   - 训练模型输出两部分内容：**[解题过程]** + **[自我评价]**。
   - 模型必须诚实地评估自己的解题过程。

2. **独特的奖励函数 (Reward Function)：**
   - 如果模型写出了错误的证明，但自己在"自我评价"中**诚实地指出了错误**，它会获得奖励。
   - 如果模型写错了但**硬说自己是对的**，会受到严厉惩罚。
   - **目的：** 激励模型在提交最终答案前，尽可能多地通过"自查"来发现并解决问题，而不是盲目试错。

#### C. 协同进化与自动化标注 (Synergy & Auto-labeling)

为了持续变强，验证器和生成器需要互相促进。

1. **生成数据：** 随着生成器变强，它会生成更难、更复杂的证明，这对验证器提出了更高要求。
2. **自动化标注 (Scaling Verification Compute)：**
   - **不再依赖人类**专家标注海量数据。
   - 对于一个新生成的证明，让验证器进行 **N 次** 独立评估。
   - 利用 Meta-Verifier 检查这些评估的一致性。如果大多数评估都认为证明有错且理由成立，系统就自动将该证明标记为"有缺陷"。
   - 这种自动化流水线极大地扩展了训练数据的规模和难度。

### 实战策略：如何最大化解题性能

在实际做题时，DeepSeekMath-V2 采用了两种策略来最大化性能：

1. **顺序修正 (Sequential Refinement)：**
   - 模型生成证明 -> 自我验证发现漏洞 -> 根据漏洞重新生成 -> 再验证。
   - 不断循环，直到模型认为证明完美或达到最大次数。

2. **高算力搜索 (High-Compute Search)：**
   - 对于极难的题目（如 IMO），通过大规模并行生成（例如初始生成 64 个样本），并对每个样本进行多次验证。
   - 筛选出评分最高的证明，把它的"验证反馈"作为提示词，让模型进一步优化。

### 模拟“整体代码审查”的 Agent Workflow 构想

**DeepSeekMath-V2 的核心贡献在于证明了 LLM 可以通过"自我反思"来掌握严密的数学逻辑。** 它不再只是模仿人类的答案，而是模仿了人类数学家**"写证明-检查-发现漏洞-修正"的完整思维过程**。这为解决复杂的开放性问题 (Open Problems) 指明了一条可行的道路。

借鉴 DeepSeekMath-V2 的"生成-验证"思想，我们可以设计一个高度模拟真实世界软件开发中"整体代码审查" (Holistic Code Review) 的自动化工作流程。

#### 核心角色

1. **生成 Agent (Generator):**
   负责根据需求编写初始代码。其目标是产出一份能通过"资深技术评审"的、高质量的代码提交。

2. **验证 Agent (Verifier):**
   扮演经验丰富的"资深技术评审"角色，对代码进行全面、综合的评估。

3. **元验证器 (Meta-Verifier):**

   扮演"评审主席"或"技术委员会"的角色。它不直接审查代码，而是**审查其他验证 Agent 提交的"评审报告"**，通过比对和共识算法，生成一份最终的、高可信度的权威评审结论。


#### 完整工作流程

**第一步：代码生成 (Code Generation)**

- 生成 Agent 根据给定的功能需求、技术规格等输入，完成初始版本的代码编写。

**第二步：综合评估与一致性校验 (Comprehensive Evaluation & Consistency Check)**

- 验证 Agent 从多个维度对提交的代码进行全面审查，并对发现的问题进行分类和优先级排序。

- **评估维度与标准：**
  - **致命错误 (Blockers / Critical):** *必须立即修复，否则代码无法合并。*
    - **功能错误：** 代码无法运行、编译失败、核心单元测试未通过。
    - **重大安全漏洞：** 存在 SQL 注入、硬编码密钥、严重的数据越权风险。
    - **核心业务逻辑错误：** 未能满足需求文档中的关键要求。

  - **重要建议 (Major Issues):** *强烈建议修复，以保证系统的长期健康度。*
    - **可维护性差：** 复杂的"面条代码"、缺乏注释、模块划分不合理。
    - **性能隐患：** 明显的低效算法、数据库查询可能存在 N+1 问题。
    - **不符合架构规范：** 未遵循团队约定的设计模式、错误地调用了服务。

  - **风格建议 (Minor / Nitpicks):** *建议修改以保持代码库一致性，但非强制。*
    - **代码规范：** 命名不规范、格式化问题、不符合团队编码风格。
    - **最佳实践：** 可以用更简洁、更现代的语法糖来替代。

- **增强评估可靠性：**
  - 为了防止单个验证器出现误判或"幻觉"，系统可让多个独立的验证 Agent **进行 N 次评估**。
  - 引入一个 **Meta-Verifier（元验证器）** 来检查这 N 次评估的一致性。如果大多数评估都认为代码存在某个缺陷且理由相似，系统就采纳该结论，从而大大提高评审结果的准确性和可靠性。

**第三步：生成结构化评审报告 (Generate Structured Review Report)**

- 验证 Agent 将所有评估结果汇总成一份包含明确优先级的结构化报告（如 JSON 格式），清晰地反馈给生成 Agent。

- **示例报告：**
    ```json
    {
      "blockers": [
        "单元测试 'test_user_login' 失败: 预期状态码 200，实际为 403"
      ],
      "majors": [
        "'get_user_data' 函数过于复杂，建议拆分成多个辅助函数",
        "API 未进行权限校验，存在数据越权风险"
      ],
      "minors": [
        "变量 'userList' 命名不符合团队的驼峰命名规范"
      ]
    }
    ```

**第四步：优先级驱动的修改 (Priority-Driven Modification)**

- 生成 Agent 接收并解析这份结构化报告。
- 它会严格按照优先级顺序进行代码修改：
  1. **首先，集中解决所有的 `blockers`**，因为这些是功能的根本性问题。
  2. 在所有 `blockers` 都清除后，接着处理 `majors`，以提升代码质量。
  3. 最后，处理 `minors`，进行代码风格的优化。

**第五步：迭代循环直至通过 (Iterate Until Approved)**

- 修改后的代码被重新提交给验证 Agent，重复执行**第二至第四步**。
- 这个"编码-审查-修改"的循环会持续进行，直到评审报告中不再包含任何 `blockers`，并且 `majors` 的数量满足预设的质量门槛，流程才结束，产出最终合格的代码。